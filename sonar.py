# -*- coding: utf-8 -*-
"""Sonar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/179f__B5gcqKlUtsYjRCb0pksV3P-L3y1
"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

#fix random seed reproductivity
seed = 7
np.random.seed(seed)

import io
from google.colab import files
uploaded = files.upload()
df = pd.read_csv(io.StringIO(uploaded['sonar.all-data'].decode('utf-8')))
df

df.head()

dataset = df.values
print(dataset[0])
print(dataset.ndim)
print(dataset.shape)
print(type(dataset))

x = dataset[:, 0:60].astype(float)
y= dataset[:,60]
print(x[0])
print(y)
print(x.shape)
print(y.shape)

LE = LabelEncoder()
LE.fit(y)
y_encode = LE.transform(y)
y_encode

#To create a baseline
from keras import optimizers
def create_baseline():
  network = Sequential()
  network.add(Dense(60, activation = 'relu' , input_shape = (60,)))
  network.add(Dense(1, activation ='sigmoid'))
  network.compile(optimizer = 'rmsprop' , loss = 'binary_crossentropy' , metrics=['accuracy'])
  return network

#evaluate
estimator = KerasClassifier(build_fn = create_baseline , epochs = 100 , batch_size = 5 , verbose =0)
kfold = StratifiedKFold(n_splits = 10 , shuffle = True , random_state = seed)
results = cross_val_score(estimator , x , y_encode , cv= kfold)
print("Results : %2f%%(%2f%%)" %(results.mean()*100, results.std()*100))

#Evaluate Baseline model
np.random.seed(seed)
estimators =[]
estimators.append(('standarize' , StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn = create_baseline , epochs=100 , batch_size =5 , verbose = 0)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits = 10, shuffle= True , random_state= seed)
results = cross_val_score(pipeline , x , y_encode , cv= kfold)
print("Standarized : %2f%%(%.2f%%)" %(results.mean()*100 , results.std()*100))

#TO EVALUATE THE SMALLER NETWORK

def create_smaller():
  small_network = Sequential()
  small_network.add(Dense(30, activation= 'relu',input_shape=(60,)))
  small_network.add(Dense(1, activation = 'sigmoid'))
  small_network.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy' , metrics =['acc'])
  return small_network

#Run
estimators =[]
estimators.append(('standarize' , StandardScaler()))
estimators.append(('mlp' , KerasClassifier(build_fn = create_smaller , epochs =100 , batch_size = 5 , verbose=0)))
pipeline = Pipeline(estimators)
kfold = StratifiedKFold(n_splits =10 , shuffle = True , random_state = seed)
results = cross_val_score(pipeline , x , y_encode , cv= kfold)
print('smaller: %2f%%(%2f%%)' % (results.mean()*100 , results.std()*100))

#TO CREATE A larger network
from keras import optimizers 
def create_larger():
  large_network = Sequential()
  large_network.add(Dense(60, activation = 'relu' , input_shape =(60,)))
  large_network.add(Dense(30, activation='relu'))
  large_network.add(Dense(1, activation='sigmoid'))
  large_network.compile(optimizer = 'rmsprop' , loss = 'binary_crossentropy' , metrics=['acc'])
  return large_network

#RUn 
estimators = []
estimators.append(('standarize' , StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn = create_larger , epochs = 100 , batch_size = 5 , verbose =0 )))
pipeline = Pipeline(estimators)
kfold= StratifiedKFold(n_splits = 10 , shuffle = True , random_state= seed)
results = cross_val_score(pipeline , x , y_encode , cv= kfold)
print("larger : %2f%% (%2f%%)" % (results.mean()*100 , results.std()*100))

#Create baseline 
from keras import optimizers 
from keras import regularizers
def create_final():
  final_network = Sequential()
  final_network.add(Dense(30, activation= 'relu' , input_shape=(60,)))
  final_network.add(Dense(1, activation='sigmoid'))
  final_network.compile(optimizer = 'rmsprop' , loss= 'binary_crossentropy' , metrics=['acc'])
  return final_network

#Evaluate Model

estimator = KerasClassifier(build_fn = create_final , epochs=100, batch_size = 5 , verbose=0)
kfold= StratifiedKFold(n_splits= 10 , shuffle=True , random_state=seed)
results = cross_val_score(estimator , x , y_encode , cv = kfold)
print("Results : %2f%%(%2f%%)" %(results.mean()*100 , results.std()*100))

#Functional API
from keras.layers import Input , Dense
from keras.models import Model
def functional_model():
  inputs = Input(shape=(60,))
  z = Dense(30, activation='relu')(inputs)
  outputs= Dense(1, activation='sigmoid')(z)
  model= Model(inputs, outputs)
  model.compile(optimizer = 'Adam', loss='binary_crossentropy' , metrics=['acc'])
  return model

#Run the model
estimators=[]
estimators.append(('standarize' , StandardScaler()))
estimators.append(('mlp' , KerasClassifier(build_fn= functional_model , epochs = 100 , batch_size=5 , verbose=0)))
pipeline = Pipeline(estimators)
kfold= StratifiedKFold(n_splits= 10 , shuffle=True , random_state= seed)
results= cross_val_score(pipeline, x , y_encode , cv= kfold)
print('funcational api: %2f%%(%2f%%)'%(results.mean()*100 , results.std()*100))

#Build Model with Model subclassing 
import tensorflow as tf
class SubclassModel(tf.keras.Model):
  def __init__(self):
    super(SubclassModel, self).__init__()
    self.dense1= tf.keras.layers.Dense(60, activation= tf.nn.relu)
    self.dense2= tf.keras.layers.Dense(30, activation= tf.nn.relu)
    self.dense3= tf.keras.layers.Dense(1, activation= tf.nn.sigmoid)
  def call(self,inputs):
    x = self.dense1(inputs)
    x= self.dense2(x)
    return self.dense3(x)
def finalModel():
  model = SubclassModel()
  model.compile(loss= 'binary_crossentropy' , optimizer = 'adam' , metrics= ['acc'])
  return model

estimators =[]
estimators.append(('standarize' , StandardScaler()))
estimators.append(('mlp' , KerasClassifier(build_fn= finalModel , epochs = 100 , batch_size= 5 , verbose=0)))
pipeline= Pipeline(estimators)
kfold= StratifiedKFold(n_splits = 10 , shuffle= True , random_state= seed)
results = cross_val_score(pipeline , x , y_encode, cv=kfold) 
print('Subclass larger Model : %2f%%(%2f%%)'% (results.mean()*100, results.std()*100))

#without scikit learn

print(x)
print(y_encode)

complete_data = dataset.copy()
np.random.shuffle(complete_data)
data = complete_data[:,0:60].astype('float32')
labels= complete_data[:,60]
le = LabelEncoder()
le.fit(labels)
labels_encode= le.transform(labels).astype('float32')
print(labels_encode)
print(data.dtype)
train_data= data[:140]
test_data=data[140:]
train_labels= labels_encode[:140]
test_labels= labels_encode[140:]
print(train_data.shape)
print(test_data.shape)
print(train_labels.shape)
print(test_labels.shape)
print(train_labels[0])

def build_model():
  model= Sequential()
  model.add(Dense(60, activation='relu' , input_shape=(60,)))
  model.add(Dense(30, activation='relu'))
  model.add(Dense(1 , activation='sigmoid'))
  model.compile(optimizer='Adam' , loss= 'binary_crossentropy' , metrics=['acc'])
  return model

k=4
num_val_samples= len(train_data)//k
num_epochs= 100

for i in range(k):
  print('processing fold#' , i)
  val_data = train_data[i* num_val_samples: (i+1) * num_val_samples]
  val_labels = train_labels[i* num_val_samples : (i+1) * num_val_samples]
  partial_train_data= np.concatenate([train_data[:i* num_val_samples], train_data[(i+1)*num_val_samples:]], axis=0)
  partial_train_labels = np.concatenate([train_labels[:i*num_val_samples], train_labels[(i+1)*num_val_samples:]], axis=0)
  
  model= build_model()
  model.fit(partial_train_data, partial_train_labels , epochs= num_epochs , batch_size=5 , verbose= 0)
  loss , acc = model.evaluate(val_data , val_labels, verbose=0)
  print(acc)

def last_model():
  model = Sequential()
  model.add(Dense(60 , activation='relu' , input_shape=(60,)))
  model.add(Dense(30, activation='relu'))
  model.add(Dense(1 , activation='sigmoid'))
  model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=['acc'])
  return model

model= last_model()
model.fit(train_data , train_labels , epochs= 100 , batch_size= 5 , verbose=0)

history= model.evaluate(test_data , test_labels) 
history

